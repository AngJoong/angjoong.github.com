---
layout: post
title:  "8. 스파크 최적화 및 디버깅"
date:   2016-10-05 20:38:00 +0000
tags: ['spark']
author: "AngJoong"
---

* 스파크 애플리케이션의 성능에 대한 이해와 설정 방법

# SparkConf 설정
* SparkConf는 스파크의 주된 **설정 메커니즘**이다.
* 스파크에서의 **최적화라는 것은 단순하게 실행 설정을 바꾸는 것**만을 뜻하기도 한다.
* SparkContext의 생성자에 SparkConf가 넘겨지고 나면 설정 수정이 불가능 하다.
* 유일하게 **로컬 저장 디렉터리 설정**은 SparkConf로 설정이 불가능하다. - 물리적인 서버들 간에 위치가 달라질 수 있어
* 설정 **우선 순위**
	1. 사용자 코드 SparkConf
	2. spark-submit 플레그
	3. spark-submit 파일
	4. 기본 값

## 1. 사용자 코드에서 설정
* 사용자가 재정의해서 쓸 수 있는 설정들에 대한 키와 쌍들을 갖고 있다.
* `.set()`을 호출하여 설정값들을 추가 한다.
* 일반적인 설정을 셋팅하는 메서드들을 제공한다. - `setAppName, setMaster()...`

```java
// conf 생성
SparkConf conf = new SparkConf();
conf.set("spark.app.name", "My Spark App"); // .setAppName("My Spark App")
conf.set("spark.master", "local[4]"); // .setMaster("local[4]")
conf.set("spark.ui.port", "36000");

JavaSparkContext sc = new JavaSparkContext(conf);
```

## 2. spark-submit에서 설정
* 애플리케이션을 **실행할때 동적으로 설정** 값을 줄 수 있다.
* spark-submit 도구는 내장된 가장 **일반적인 스파크 설정** 및 **범용적인 플래그**(`--conf`) 모두 지원한다.
* 설정값을 **파일에서 읽는 것**도 지원한다. 기본 설정 파일은 `conf/spark-defaults.conf`이다.

```bash
# 1. 플래그를 사용하여 설정
$ bin/spark-submit \
  --class com.example.MyApp \
  --master local[4] \
  --name "My Spark App" \
  --conf spark.ui.port=36000 \
  myApp.jar

# 2. 파일을 읽어 설정
$ bin/spark-submit \
  --class com.example.MyApp \
  --properties-file my-config.conf \ #설정 파일 경로 설정
  myApp.jar
```


# 성능에 대한 이해
* 스파크를 최적화하고 디버깅하기 위해서 시스템의 내부 설계를 이해해야 한다.

## 작업, 태스크, 작업 단계 - 실행을 구성하는 것
2. 내부적으로 정의된 지향성 비순환 그래프(DAG, Directed Acyclic Graph)가 생긴다.
3. 각 RDD들은 부모 RDD를 가리키는 포인터를 가진다.
4. 부모에 대한 포인터는 모든 조상 RDD를 추적할 수 있게 해준다.


```txt
    ## input.txt ##
    INFO This is a message with content
    INFO This is some other content

    INFO Here are more messages
    WARN This is a warninig

    ERROR Something bad happened
    WARN More details on the bad thing
    INFO back to normal messages
```

```scala
// 입력 파일 읽기
scala> val input = sc.textFile("input.txt");
// 빈라인 제거 및 단어 분리
scala> val tokenized = input.
		filter(line => line.size > 0).
        map(line => line.split(" "))
// 첫번째 단어(로그 레벨) 추출 및 카운팅
scala> val counts = tokenized.
		map(words => (words(0), 1)).
        reduceByKey{ (a, b) => a + b}

scala> counts.collect()
```
1. 셸에서 위 코드를 실행하게 되면 어떤 액션도 수행하지 않는다.
2. 대신 내부적으로 정의된 지향성 비순환 그래프(DAG, Directed Acyclic Graph)가 생긴다.
3. DAG는 액션을 수행할때 사용된다.
4. 스파크 스케쥴러는 액션을 수행할 때 필요한 RDD 연산의 물리적 실행 계획을 만든다.
5. RDD에 `collect()`(액션)를 호출할때 RDD의 모든 파티션들이 실체화되고 드라이버 프로그램으로 전송된다.
6. 스파크 스케쥴러는 마지막으로 연산되는 RDD(예제의 경우 counts)에서 시작하여 연산해야 할 것을 역추적 한다.
