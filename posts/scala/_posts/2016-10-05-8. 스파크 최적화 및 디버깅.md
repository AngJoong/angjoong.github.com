---
layout: post
title:  "8. 스파크 최적화 및 디버깅"
date:   2016-10-05 20:38:00 +0000
description: 스파크 애플리케이션의 성능에 대한 이해와 설정 방법
tags: ['Spark']
author: "AngJoong"
---

# SparkConf 설정
* SparkConf는 스파크의 주된 **설정 메커니즘**이다.
* 스파크에서의 **최적화라는 것은 단순하게 실행 설정을 바꾸는 것**만을 뜻하기도 한다.
* SparkContext의 생성자에 SparkConf가 넘겨지고 나면 설정 수정이 불가능 하다.
* 유일하게 **로컬 저장 디렉터리 설정**만은 SparkConf로 설정이 불가능하다.

## 1. 사용자 코드에서 설정
* 사용자가 재정의해서 쓸 수 있는 설정들에 대한 키와 쌍들을 갖고 있다.
* `.set()`을 호출하여 설정값들을 추가 한다.
* 일반적인 설정을 셋팅하는 메서드들을 제공한다. - `setAppName, setMaster()...`

##### <예제 8-1> SparkConf 설정
```java
// conf 생성
SparkConf conf = new SparkConf();
conf.set("spark.app.name", "My Spark App"); // .setAppName("My Spark App")
conf.set("spark.master", "local[4]"); // .setMaster("local[4]")
conf.set("spark.ui.port", "36000");

JavaSparkContext sc = new JavaSparkContext(conf);
```

## 2. spark-submit에서 설정
* 애플리케이션을 **실행할때 동적으로 설정** 값을 줄 수 있다.
* spark-submit 도구는 내장된 가장 **일반적인 스파크 설정** 및 **범용적인 플래그**(`--conf`) 모두 지원한다.
* 설정값을 **파일에서 읽는 것**도 지원한다. 기본 설정 파일은 `conf/spark-defaults.conf`이다.

##### <예제 8-2> spark-submit 설정
```bash
# 1. 플래그를 사용하여 설정
$ bin/spark-submit \
  --class com.example.MyApp \
  --master local[4] \
  --name "My Spark App" \
  --conf spark.ui.port=36000 \
  myApp.jar

# 2. 파일을 읽어 설정
$ bin/spark-submit \
  --class com.example.MyApp \
  --properties-file my-config.conf \ #설정 파일 경로 설정
  myApp.jar
```

## 3. 설정 우선 순위
* 동일한 설정값이 여러곳에서 사용되는 경우 스카프 설정 우선순위를 따른다.
> 사용자 코드 SparkConf -> spark-submit 플레그 -> spark-submit 파일 -> 기본 값


# 실행 구성 - 작업(Job), 태스크(Task), 작업 단계(Stage)
스파크를 최적화하고 디버깅하기 위해서 시스템의 내부 설계를 이해 한다.

##### <예제 8-3> 로그 레벨 카운팅
``` scala
// 입력 파일 읽기
scala> val input = sc.textFile("input.txt"); // input.txt - line ex> INFO back to normal messages
// 빈라인 제거 및 단어 분리
scala> val tokenized = input.
		filter(line => line.size > 0).
        map(line => line.split(" "))
// 첫번째 단어(로그 레벨) 추출 및 카운팅
scala> val counts = tokenized.
		map(words => (words(0), 1)).
        reduceByKey{ (a, b) => a + b}

scala> counts.collect()
```

## 가계도(Lineage graph) - DAG
* RDD는 내부적으로 정의된 **가계도**라고 불리는 지향성 비순환 그래프(DAG, Directed Acyclic Graph) 갖는다.
* 각 RDD들은 부모 RDD를 가리키는 포인터를 가지며 이를 활용해 모든 조상 RDD를 추적할 수 있다.

##### <예제 8-4> RDD 가계도 시각화
```scala
scala> input.toDebugString
res85: String =
(2) input.text MappedRDD[292] at textFile at <console>:13
 -  input.text HadoopRDD[291] at textFile at <console>:13

scala> counts.toDebugString
res85: String =
(2) ShuffledRDD[296] at reduceByKey at <soncole>:17
 +-(2) MappedRDD[295] at map at <console>:17
    |  FilteredRDD[294] at filter at <console>:15
    |  MappedRDD[293] at map at <console>:15
    |  input.text MappedRDD[292] at textFile at <console>:13
    |  input.text HadoopRDD[291] at textFile at <console>:13
```
* 위 가계도를 통해 어떤 함수가 어떤 RDD를 만들었는지 확인할 수 있다.
> `.textFile()` -> HadoopRDD -> MappedRDD
* 여러 개의 RDD를 하나의 작업 단계(Stage)로 합치는 경우 존재
	* 파이프 라이닝: RDD들이 데이터 이동 없이 부모에서부터 연산이 가능할때 발생
	* 건너뛰기(short-circuit): 캐싱된 부분은 생략하고 영속화 되어 있는 RDD기준으로 연산한다.
	* 이미 이전에 실행된 셔플링으로 인해 RDD의 데이터가 남아 있는 경우
* 동일한 들여쓰기 단계에 따라 물리 단계에서 합쳐지는 것을 나타낸다.
> 1. counts RDD연산시 단 2단계의 물리적 단계 존재(들여쓰기)
> 2. filter()와 map()등 여러 연산이 순서대로 이루어지는 파이프라이닝



##### <예제 8-5> RDD 모으기
```scala
scala> counts.collect()
res86: Array[(String, Int)] = Array((ERROR,1), (INFO,4), (WARN,2))
```
* 스파크의 스케줄러는 연산한 마지막 RDD`(counts)`부터 가계도를 역으로 추적해 물리적 실행계획을 만든다.
* **<예제 8-5>**의 `collect()`를 호출할때 물리적 실행계획을 통해 RDD의 모든 파티션들이 실체화되고 드라이버 프로그램으로 전송된다.

### 작업 단계(Stage)
* 액션을 호출할 때마다 하나 이상의 작업 단계로 구성된 작업(Job)을 만든다.
* 물리적 작업 단계는 실행되는 데이터 파티션은 서로 다르지만 같은 일을 수행하는 태스크(Task)들을 실행시킨다.

#### 태스크(Task) 수행 순서
1. 데이터를 가져온다.
	1. 저장 장치: RDD가 입력 RDD인 경우
	2. 존재하는 RDD: 캐시된 데이터에 대해 작업 단계가 수행 될 경우
	3. 셔플 결과물
2. RDD 연산들을 수행한다.
3. 결과를 셔플, 외부 저장 장치에 쓰거나 드라이버에 되돌려 준다.
